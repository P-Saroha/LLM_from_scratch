{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd4ce0f",
        "outputId": "66d24d97-1423-455a-9180-5b705be4edf1"
      },
      "source": [
        "# Install specific compatible versions of gensim, numpy, and scipy\n",
        "!pip install gensim==4.3.3 numpy==1.26.4 scipy==1.11.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.3) (7.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9987ac6a",
        "outputId": "cc38c760-c1bb-42fb-a823-a3ad4f1233bd"
      },
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb06ec3"
      },
      "source": [
        "# Example of a word as a vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68e70aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45086a11-6450-4b95-c01b-688fde1de20d"
      },
      "source": [
        "word_vectors = model\n",
        "\n",
        "# Let us look how the vector embedding of a word looks like\n",
        "print(word_vectors['computer'])  # Example: Accessing the vector for the word 'computer'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
            " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
            "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
            "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
            " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
            " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
            "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
            "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
            " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
            " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
            " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
            "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
            " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
            "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
            "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
            "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
            " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
            " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
            "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
            "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
            "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
            "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
            " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
            " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
            "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
            "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
            " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
            "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
            " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
            " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
            " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
            " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
            "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
            " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
            "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
            "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
            " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
            " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
            " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
            " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
            " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
            " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
            " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
            " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
            "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
            " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
            "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
            "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
            " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
            "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
            " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
            "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
            " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
            " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
            " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
            " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
            " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
            " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
            " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
            "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
            "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
            " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
            "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
            " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
            "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
            " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
            "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
            " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
            " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
            " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
            "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
            "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
            "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
            "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
            "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70967fe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7c4e73-ffc4-4b46-b8aa-d6b7fea0a05c"
      },
      "source": [
        "print(word_vectors['cat'].shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db35a185"
      },
      "source": [
        "# Similar words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c47a45"
      },
      "source": [
        "# King + Woman - Man = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c997a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be58db2-78d0-4d96-bd41-c8208a0a4233"
      },
      "source": [
        "# Example of using most_similar\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c40ad8"
      },
      "source": [
        "# Let us check the similarity b/w a few pair of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed5648d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b27de9-1915-4a19-db4d-92d8133b88fe"
      },
      "source": [
        "# Example of calculating similarity\n",
        "print(word_vectors.similarity('woman', 'man'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('nephew', 'niece'))\n",
        "print(word_vectors.similarity('paper', 'water'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.76640123\n",
            "0.6510957\n",
            "0.7643474\n",
            "0.8543272\n",
            "0.7594367\n",
            "0.11408084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b41c7d0b"
      },
      "source": [
        "# Most similar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16d35e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd615c1-145a-49aa-f83b-19d1211d3edf"
      },
      "source": [
        "print(word_vectors.most_similar(\"tower\", topn=5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('towers', 0.8531750440597534), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.594687819480896), ('responded_Understood_Atlasjet', 0.5931612253189087)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5d59e42"
      },
      "source": [
        "# Now let us see the vector similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f3c22b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72664a1-f1e7-4bef-9fbe-7747456851ee"
      },
      "source": [
        "import numpy as np\n",
        "# Words to compare\n",
        "word1 = 'man'\n",
        "word2 = 'woman'\n",
        "\n",
        "word3 = 'semiconductor'\n",
        "word4 = 'earthworm'\n",
        "\n",
        "word5 = 'nephew'\n",
        "word6 = 'niece'\n",
        "\n",
        "# Calculate the vector difference\n",
        "vector_difference1 = model[word1] - model[word2]\n",
        "vector_difference2 = model[word3] - model[word4]\n",
        "vector_difference3 = model[word5] - model[word6]\n",
        "\n",
        "# Calculate the magnitude of the vector difference\n",
        "magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
        "magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
        "magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
        "\n",
        "\n",
        "# Print the magnitude of the difference\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word1, word2, magnitude_of_difference1))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word3, word4, magnitude_of_difference2))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word5, word6, magnitude_of_difference3))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The magnitude of the difference between 'man' and 'woman' is 1.73\n",
            "The magnitude of the difference between 'semiconductor' and 'earthworm' is 5.67\n",
            "The magnitude of the difference between 'nephew' and 'niece' is 1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "yzEec0xLDw2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: Token ID → Embedding Vector in PyTorch\n",
        "We’ll simulate a simple case with:\n",
        "\n",
        "A vocabulary of 10 tokens (IDs from 0 to 9)\n",
        "\n",
        "Each token represented by a 4-dimensional vector"
      ],
      "metadata": {
        "id": "6WuQmgg3D1C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "5MmpoQgtDwk_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create an Embedding Layer\n",
        "\n"
      ],
      "metadata": {
        "id": "C7BpPW5vE1MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding layer for 10 tokens, each with 4 features\n",
        "embedding = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
        "#This creates a lookup table (a matrix of size 10 × 4) where each row is a vector for a token ID from 0 to 9."
      ],
      "metadata": {
        "id": "MpqeJZ7LEvzZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define Token IDs (Input)"
      ],
      "metadata": {
        "id": "rHiViE0BFDoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we want embeddings for token IDs: 1, 2, and 4\n",
        "token_ids = torch.tensor([1, 2, 4])\n"
      ],
      "metadata": {
        "id": "ZSlXhxOzFAWf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Pass Token IDs Through the Embedding Layer"
      ],
      "metadata": {
        "id": "umA3FUClFKHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding vectors\n",
        "embedded_vectors = embedding(token_ids)\n",
        "# Now, embedded_vectors contains 3 vectors (one for each token), each of size 4.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ci_j9u8gFGxg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 5: Print the Result"
      ],
      "metadata": {
        "id": "yrxHtrPwFTM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\", token_ids)\n",
        "print(\"Embedding Vectors:\\n\", embedded_vectors)\n"
      ],
      "metadata": {
        "id": "7_nT_JxYFP5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc263e24-ce36-46d9-b7bf-77e87b309ff6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: tensor([1, 2, 4])\n",
            "Embedding Vectors:\n",
            " tensor([[-1.0913, -0.0933,  0.2355,  1.1382],\n",
            "        [ 0.1318,  0.7752,  0.7384, -0.3266],\n",
            "        [ 1.0427,  0.2396, -1.6096, -2.1470]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View All 10 Embeddings\n",
        "print(\"All embeddings:\\n\", embedding.weight.data)\n"
      ],
      "metadata": {
        "id": "UPc7LMH-FcMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9c0262-48a4-4e1f-c4e9-76c5f090421a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All embeddings:\n",
            " tensor([[-0.4529, -0.0054, -0.1694, -0.3142],\n",
            "        [-1.0913, -0.0933,  0.2355,  1.1382],\n",
            "        [ 0.1318,  0.7752,  0.7384, -0.3266],\n",
            "        [ 2.6535,  0.1505, -0.2863, -0.2661],\n",
            "        [ 1.0427,  0.2396, -1.6096, -2.1470],\n",
            "        [ 0.6722,  0.1205,  1.2672, -1.1633],\n",
            "        [-0.3426, -1.8629,  0.2311, -0.8454],\n",
            "        [-0.9374, -0.0618,  1.6912, -1.8479],\n",
            "        [-1.3406, -0.4582,  1.1096,  1.3724],\n",
            "        [-0.3250, -1.1878,  1.5225, -0.7665]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodding"
      ],
      "metadata": {
        "id": "lzV2o1IxNFWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset with Sliding Window\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "\n",
        "            if len(input_chunk) == max_length and len(target_chunk) == max_length:\n",
        "                self.input_ids.append(torch.tensor(input_chunk))\n",
        "                self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "RyXaMkNnQmgP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataloader Function\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=False, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    print(f\"Dataset created with {len(dataset)} samples\")\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                      drop_last=drop_last, num_workers=num_workers)\n"
      ],
      "metadata": {
        "id": "5XCQuwtBZ100"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Text & Create Dataloader\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "max_length = 4         # Sequence length\n",
        "batch_size = 8         # Number of sequences per batch\n",
        "stride = 4             # No overlap\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=batch_size, max_length=max_length,\n",
        "    stride=stride, shuffle=False, drop_last=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDpAQkE0Z5y2",
        "outputId": "93a40a15-53e7-4ae3-8259-ef9d26fbb28b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 1286 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(dataloader.dataset) == 0:\n",
        "    print(\" Dataset is empty. Try smaller max_length or stride.\")\n",
        "else:\n",
        "    data_iter = iter(dataloader)\n",
        "    inputs, targets = next(data_iter)\n",
        "\n",
        "    print(\" Token IDs:\\n\", inputs)\n",
        "    print(\" Inputs shape:\", inputs.shape)  # ➤ Shape: [8, 4]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptcMI2GsZ7g8",
        "outputId": "430e857a-44c5-4145-95da-65435cc65c99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            " Inputs shape: torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token Embedding Layer\n",
        "vocab_size = 50257       # GPT-2 vocab size\n",
        "output_dim = 256         # Embedding dimension\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "\n",
        "print(\"Token embeddings shape:\", token_embeddings.shape)  # ➤ Shape: [8, 4, 256]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BebEu9wtaAS6",
        "outputId": "4cd36bb3-93df-44d9-a97c-393232e785a7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embeddings shape: torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Positional Embedding Layer\n",
        "pos_embedding_layer = torch.nn.Embedding(max_length, output_dim)\n",
        "\n",
        "# Positions: [0, 1, 2, 3]\n",
        "pos_ids = torch.arange(max_length)              # ➤ Shape: [4]\n",
        "pos_embeddings = pos_embedding_layer(pos_ids)   # ➤ Shape: [4, 256]\n",
        "print(\" Positional embeddings shape:\", pos_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwQ8tCBmaEY6",
        "outputId": "2786ef03-7262-4a75-a56f-5f75681d96f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Positional embeddings shape: torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Token + Positional Embeddings\n",
        "# Broadcast position embeddings across batch (8)\n",
        "pos_embeddings = pos_embeddings.unsqueeze(0)          # ➤ Shape: [1, 4, 256]\n",
        "input_embeddings = token_embeddings + pos_embeddings  # ➤ Shape: [8, 4, 256]\n",
        "\n",
        "print(\" Input embeddings shape (final):\", input_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oideo2d9bDza",
        "outputId": "db468a98-be75-4103-8394-36f18beab44f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input embeddings shape (final): torch.Size([1, 8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Component                     | Shape         | Description                      |\n",
        "| ----------------------------- | ------------- | -------------------------------- |\n",
        "| `inputs`                      | `[8, 4]`      | Token IDs                        |\n",
        "| `token_embeddings`            | `[8, 4, 256]` | Token ID → vector                |\n",
        "| `pos_embeddings`              | `[4, 256]`    | One vector per position          |\n",
        "| `pos_embeddings.unsqueeze(0)` | `[1, 4, 256]` | Broadcasted for batch            |\n",
        "| `input_embeddings`            | `[8, 4, 256]` | Final input to Transformer block |\n"
      ],
      "metadata": {
        "id": "Ddhqvr3_aHXl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTxdf02baH11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}